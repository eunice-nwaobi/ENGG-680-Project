{"cells":[{"cell_type":"markdown","metadata":{"id":"aE_sNMOuDJgy"},"source":["# Libraries"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":166,"status":"ok","timestamp":1731433260511,"user":{"displayName":"eunice","userId":"09141754589569284162"},"user_tz":420},"id":"aaAGVecZOX6o"},"outputs":[],"source":["import torch\n","import torch.nn  as nn\n","import torch.nn.functional as F\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import numpy as np\n","from sklearn.metrics import confusion_matrix, classification_report"]},{"cell_type":"markdown","metadata":{"id":"SByvRgG7DP38"},"source":["# Model"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":175,"status":"ok","timestamp":1731433261660,"user":{"displayName":"eunice","userId":"09141754589569284162"},"user_tz":420},"id":"OO20FVw4o2_E"},"outputs":[],"source":["class aMeijerNet(nn.Module):\n","  def __init__(self):\n","    super(aMeijerNet, self).__init__()\n","    # input layer - 288x288x3\n","    # convolution layer - kernel 5x5 depth 32 ELU activation\n","    self.conv = nn.Conv2d(3, 32, kernel_size = 5, padding=2)\n","    self.elu = nn.ELU()\n","    # pooling layer - kernel 2x2 stride 2x2\n","    self.pool = nn.MaxPool2d(kernel_size = 2, padding=1)\n","\n","    # convolution layer 2 - kernel 5x5 depth 32 ELU activation\n","    self.conv2 = nn.Conv2d(32, 32, kernel_size=5, padding=2)\n","    self.elu2 = nn.ELU()\n","    # pooling layer 2 - kernel 2x2 stride 2x2\n","    self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n","\n","    # convulation layer 3 - kernel 5x5 depth 32 ELU activation\n","    self.conv3 = nn.Conv2d(32, 32, kernel_size=5, padding=2)\n","    self.elu3 = nn.ELU()\n","    # pooling layer 3 - kernel 2x2 stride 2x2\n","    self.pool3 = nn.MaxPool3d(kernel_size=2, stride=2)\n","\n","    # dense layer\n","    self.fc1 = nn.Linear(36, 512) # 32, 36, 36 -> 32, 36, 512\n","    # ELU activation\n","    self.elu4 = nn.ELU()\n","    # 50% dropout\n","    self.do1 = nn.Dropout2d(p=0.5)\n","\n","    # dense layer 32, 36, 512\n","    self.fc2 = nn.Linear(512, 512)\n","\n","    # ELU activation\n","    self.elu5 = nn.ELU()\n","\n","    # 50% dropout\n","    self.do2 = nn.Dropout2d(p=0.5)\n","\n","    # dense layer 32, 36, 512\n","    self.fc3 = nn.Linear(512, 512)\n","\n","    # ELU activation\n","    self.elu6 = nn.ELU()\n","\n","    # 50% dropout\n","    self.do3 = nn.Dropout2d(p=0.5)\n","\n","    # output layer - 3 outputs (defect, cracks, LRCs) - sigmoid and round for each output, binary step function, etc.\n","    self.out = nn.Linear(294912, 3)\n","    self.sigm = nn.Sigmoid()\n","\n","  def forward(self, x):\n","   # x = torch.transpose(x, 1, 0)\n","    c = self.conv(x)\n","    e = self.elu(c)\n","    p = self.pool(e)\n","\n","    c2 = self.conv2(p)\n","    e2 = self.elu2(c2)\n","    p2 = self.pool2(e2)\n","\n","    c3 = self.conv3(p2)\n","    e3 = self.elu3(c3)\n","    p3 = self.pool3(e3)\n","\n","    fc1 = self.fc1(p3)\n","    e4 = self.elu4(fc1)\n","    do1 = self.do1(e4)\n","\n","    fc2 = self.fc2(do1)\n","    e5 = self.elu5(fc2)\n","    do2 = self.do2(e5)\n","\n","    fc3 = self.fc3(do2)\n","    e6 = self.elu6(fc3)\n","    do3 = self.do3(e6)\n","\n","    out = self.sigm(self.out(torch.flatten(do3, start_dim=1, end_dim=-1)))\n","    outNum = torch.round(out, decimals=0)\n","\n","    return outNum\n","\n","# net = aMeijerNet()\n","# print(net(torch.from_numpy(imArr)))"]},{"cell_type":"markdown","metadata":{"id":"Ny_P4_EKDVGA"},"source":["# Dataset & Train/Validation"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":579,"status":"ok","timestamp":1731433264146,"user":{"displayName":"eunice","userId":"09141754589569284162"},"user_tz":420},"id":"c9D3T_qkdvOz","outputId":"c3027e42-0956-4a63-bdd8-71ab47eddec2"},"outputs":[{"output_type":"stream","name":"stdout","text":["[0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05]\n"]}],"source":["# load data\n","from PIL import Image\n","import numpy as np\n","import os\n","# rescale images to 288x288 (compression is better than expansion) - MAKE FUNCTION\n","from torch.utils.data import Dataset, DataLoader\n","from sklearn.model_selection import train_test_split\n","\n","class SewerDataset(Dataset):\n","    def __init__(self, datalocations, desiredW=288, desiredH=288):\n","        self.datalocations = datalocations\n","        self.channels, self.desiredW, self.desiredH = 3, desiredW, desiredH\n","\n","    def getArray(self, file, plot=False):\n","        im = Image.open(f\"{file}\")\n","        if plot:\n","            im\n","        w, h = im.size\n","        rw, rh = (self.desiredW/w, self.desiredH/h)\n","        imRsz = im.resize((int(im.width * rw), int(im.height * rh)), resample=1)\n","        imArr = np.array(imRsz, dtype=np.float32).transpose((2, 0, 1)) # transpose inputs Cin, Hin, Win\n","        return imArr\n","\n","    def __getitem__(self, index):\n","        self.arr = np.zeros((self.channels, self.desiredW, self.desiredH))\n","        if index == 0:\n","            self.arr = self.getArray(self.datalocations[index], True)\n","        else:\n","            self.arr = self.getArray(self.datalocations[index])\n","        label = np.array([int(self.datalocations[index][-7]),\n","                 int(self.datalocations[index][-6]),\n","                 int(self.datalocations[index][-5])])\n","        return self.arr, label\n","\n","    def __len__(self):\n","        return len(self.datalocations)\n","    # 75% training, 15% for test, 10% validation\n","\n","datalocation = \"/content/drive/MyDrive/ENGG-680-ProjectDataDestination/\"\n","pre = \"/content/drive/MyDrive/ENGG-680-Project/\"\n","files = list(set(os.listdir(datalocation)) - {'desktop.ini', 'whatever.ini'})\n","files = [datalocation+file for file in files]\n","saveFiles = f\"{pre}model/saved/\"\n","\n","_, _, filesTrain, filesTest = train_test_split(files, files, test_size=0.25, random_state=42)\n","_, _, filesTest, filesVal = train_test_split(filesTest, filesTest, test_size=10/25, random_state=42)\n","\n","trainSet = SewerDataset(filesTrain)\n","valSet = SewerDataset(filesVal)\n","criterion = torch.nn.CrossEntropyLoss()\n","trainLoader = DataLoader(dataset=trainSet, batch_size=64)\n","valLoader = DataLoader(dataset=valSet, batch_size=64)\n","learningRates = [.01, .05, .001, .005] + [.0001, .0005]  # last is 5\n","learningRates.sort()\n","print(learningRates)\n","trainErr = torch.zeros(len(learningRates))\n","valErr = torch.zeros(len(learningRates))\n","Models = {}\n","epochs = 20\n","learningRateLosses = []\n","# for i, lr in enumerate(learningRates):\n","# #     net = aMeijerNet()\n","# #     optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n","# #     for epoch in range(epochs):\n","# #         for x, y in trainLoader:\n","# #             yHat = net(x)\n","# #             loss = criterion(yHat, y.to(torch.float32))\n","# #             print(f\"Learning rate: {lr}; epoch: {epoch}; loss: {loss}.\")\n","# #             optimizer.zero_grad()\n","# #             loss.backward()\n","# #             optimizer.step()\n","# #         torch.save(net, f\"{saveFiles}net_{str(lr).replace('.', '_')}_{epoch}.pth\")\n","# #     for x, y in trainLoader:\n","# #         # training data\n","# #         yhat = net(x)\n","# #         trainLoss = criterion (yhat, y.to(torch.float32))\n","# #         trainErr[i] = trainLoss.item()\n","#     net = torch.load(f\"{saveFiles}net_{str(lr).replace('.', '_')}_{epochs-1}.pth\")\n","#     print(f\"{str(lr).replace('.', '_')} network loaded.\")\n","#     losses=[]\n","#     with torch.no_grad():\n","#       for idx, (x, y) in enumerate(valLoader):\n","#           print(x.shape)\n","#           # validation data\n","#           yhat = net(x)\n","#           valLoss = criterion(yhat, y.to(torch.float32))\n","#           valErr[i] = valLoss.item()\n","#           losses.append(valLoss)\n","#           print(f\"Loss {idx+1}: {valLoss}\")\n","#       learningRateLosses.append(losses)\n","# #         Models.append(net)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":339},"executionInfo":{"elapsed":34,"status":"error","timestamp":1731389037669,"user":{"displayName":"eunice","userId":"09141754589569284162"},"user_tz":420},"id":"8pl696Dp8moM","outputId":"87c1b467-ea48-40e4-909c-e8cbffd2164d"},"outputs":[{"ename":"AxisError","evalue":"axis 1 is out of bounds for array of dimension 1","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAxisError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-645dda40a37f>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mlrLossesArray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearningRateLosses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mlrLossesMean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlrLossesArray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlrLossesArray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlrLossesMean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mmean\u001b[0;34m(a, axis, dtype, out, keepdims, where)\u001b[0m\n\u001b[1;32m   3502\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3503\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3504\u001b[0;31m     return _methods._mean(a, axis=axis, dtype=dtype,\n\u001b[0m\u001b[1;32m   3505\u001b[0m                           out=out, **kwargs)\n\u001b[1;32m   3506\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py\u001b[0m in \u001b[0;36m_mean\u001b[0;34m(a, axis, dtype, out, keepdims, where)\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0mis_float16_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m     \u001b[0mrcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_count_reduce_items\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeepdims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhere\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mrcount\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mwhere\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mTrue\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mumr_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrcount\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Mean of empty slice.\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRuntimeWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py\u001b[0m in \u001b[0;36m_count_reduce_items\u001b[0;34m(arr, axis, keepdims, where)\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0mitems\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0max\u001b[0m \u001b[0;32min\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m             \u001b[0mitems\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_axis_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m         \u001b[0mitems\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAxisError\u001b[0m: axis 1 is out of bounds for array of dimension 1"]}],"source":["lrLossesArray = np.array(learningRateLosses)\n","lrLossesMean = np.mean(lrLossesArray, axis=1)\n","print(lrLossesArray.shape, lrLossesMean.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"paD5_DCBMa7Z"},"outputs":[],"source":["#plt.semilogx(np.array(learning_rates), trainErr.numpy(), label=\"total training loss\")\n","plt.semilogx(np.array(learningRates), lrLossesMean, label=\"total validation loss\")\n","plt.ylabel(\"Total Loss\")\n","plt.xlabel(\"Learning Rate\")\n","plt.legend()\n","plt.show()\n","\n","plt.plot(np.array(learningRates), lrLossesMean, label=\"total validation loss\")\n","plt.ylabel(\"Total Loss\")\n","plt.xlabel(\"Learning Rate\")\n","plt.legend()\n","plt.show()\n","\n","# we are looking for precision/recall given imbalanced dataset\n","\n","# for net, lr in zip(Models, learningRates):\n","#     yhat = net(valSet.x)\n","#     plt.plot(valSet.x.numpy(), yhat.detach().numpy(), label=f\"learning rate {str(lr)}\")\n","# plt.plot(valSet.x.numpy(), valSet.func.numpy())"]},{"cell_type":"markdown","metadata":{"id":"bIf71xVWDcte"},"source":["# Complete Training"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":193,"status":"error","timestamp":1731433273184,"user":{"displayName":"eunice","userId":"09141754589569284162"},"user_tz":420},"id":"8v4s2E8HDfpY","colab":{"base_uri":"https://localhost:8080/","height":428},"outputId":"25fec96f-4b76-4152-eac1-6e8c919b47b9"},"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-11-4d39dcb13e99>:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  net = torch.load(f\"{saveFiles}finalForty/net_{str(learningRate).replace('.', '_')}_{epochs-1}.pth\") #, map_location =\"cpu\")\n"]},{"output_type":"error","ename":"RuntimeError","evalue":"Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-11-4d39dcb13e99>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{saveFiles}finalForty/net_{str(learningRate).replace('.', '_')}_{epochs-1}.pth\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#, map_location =\"cpu\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda:0\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda:0\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearningRate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mmaxEpochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1338\u001b[0m                     \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1340\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m     def register_full_backward_pre_hook(\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    898\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 900\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    901\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    925\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    926\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 927\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    928\u001b[0m             \u001b[0mp_should_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1324\u001b[0m                         \u001b[0mmemory_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_to_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m                     )\n\u001b[0;32m-> 1326\u001b[0;31m                 return t.to(\n\u001b[0m\u001b[1;32m   1327\u001b[0m                     \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1328\u001b[0m                     \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    317\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"CUDA_MODULE_LOADING\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m             \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"CUDA_MODULE_LOADING\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"LAZY\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    320\u001b[0m         \u001b[0;31m# Some of the queued calls may reentrantly call _lazy_init();\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m         \u001b[0;31m# we need to just return without initializing in that case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx"]}],"source":["learningRate = .005\n","epochs=103\n","net = torch.load(f\"{saveFiles}finalForty/net_{str(learningRate).replace('.', '_')}_{epochs-1}.pth\") #, map_location =\"cpu\")\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","net.to(\"cuda:0\")\n","optimizer = torch.optim.Adam(net.parameters(), lr=learningRate)\n","maxEpochs = 200\n","for epoch in range(epochs, maxEpochs):\n","    for x, y in trainLoader:\n","        x, y = x.to(device, dtype=torch.float32), y.to(device, dtype=torch.float32)\n","        yHat = net(x)\n","        loss = criterion(yHat, y.to(torch.float32))\n","        print(f\"Learning rate: {learningRate}; epoch: {epoch}; loss: {loss}.\")\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","    if epoch>=93: torch.save(net, f\"{saveFiles}finalForty/net_{str(learningRate).replace('.', '_')}_{epoch}.pth\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B0DiT2K-EVWU"},"outputs":[],"source":["trainErr = []\n","\n","for i, (x, y) in enumerate(trainLoader):\n","    # training data\n","    x, y = x.to(device, dtype=torch.float32), y.to(device, dtype=torch.float32) #?\n","    yhat = net(x)\n","    trainLoss = criterion(yhat, y.to(torch.float32))\n","    trainErr.append(trainLoss.item())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TznZnvDdh7UU"},"outputs":[],"source":["plt.plot(list(range(len(trainErr))), trainErr)\n","plt.xlabel(\"batch\")\n","plt.ylabel(\"loss\")\n","plt.title(\"Training Loss\")"]},{"cell_type":"markdown","metadata":{"id":"y8x3rTG0ejH6"},"source":["# Testing"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UBgdUMrVeeJ3","executionInfo":{"status":"ok","timestamp":1731432624658,"user_tz":420,"elapsed":450965,"user":{"displayName":"eunice","userId":"09141754589569284162"}},"outputId":"044ba092-d7d1-485c-f233-e9646a670c40"},"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-5-c0e6f4a07c78>:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  net = torch.load(f\"{saveFiles}finalForty/net_{str(learningRate).replace('.', '_')}_{epochs-1}.pth\", map_location =\"cpu\")\n"]}],"source":["testSet = SewerDataset(filesTest)\n","learningRate = .005\n","nTest = len(filesTest)\n","epochs=103\n","net = torch.load(f\"{saveFiles}finalForty/net_{str(learningRate).replace('.', '_')}_{epochs-1}.pth\", map_location =\"cpu\")\n","predictionsCrack = np.zeros(nTest, )\n","predictionsLrc = np.zeros(nTest, )\n","predictionsDefect = np.zeros(nTest, )\n","actualCrack, actualLrc, actualDefect = np.zeros(nTest, ), np.zeros(nTest, ), np.zeros(nTest, )\n","testLoader = DataLoader(dataset=valSet, batch_size=1)\n","for i, (x, y) in enumerate(testLoader):\n","  #x, y = x.to(device, dtype=torch.float32), y.to(device, dtype=torch.float32) #?\n","  predictions = net(x)\n","  predictions = predictions.to(\"cpu\").detach().numpy()\n","  predictions = predictions.reshape(-1,)\n","  predictionsCrack[i] = predictions[0]\n","  predictionsLrc[i] = predictions[1]\n","  predictionsDefect[i] = predictions[2]\n","\n","  actualCrack[i] = y.to(\"cpu\").detach().numpy().reshape(-1,)[0]\n","  actualLrc[i] = y.to(\"cpu\").detach().numpy().reshape(-1,)[1]\n","  actualDefect[i] = y.to(\"cpu\").detach().numpy().reshape(-1,)[2]"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WtV46AT8hI1X","executionInfo":{"status":"ok","timestamp":1731432624659,"user_tz":420,"elapsed":10,"user":{"displayName":"eunice","userId":"09141754589569284162"}},"outputId":"e8484303-8576-4bb1-9435-845d6067520d"},"outputs":[{"output_type":"stream","name":"stdout","text":["For cracks:               precision    recall  f1-score   support\n","\n","         0.0       0.86      0.80      0.83      1119\n","         1.0       0.22      0.30      0.25       208\n","\n","    accuracy                           0.72      1327\n","   macro avg       0.54      0.55      0.54      1327\n","weighted avg       0.76      0.72      0.74      1327\n","\n","[[898 221]\n"," [146  62]].\n","\n","For LRC:               precision    recall  f1-score   support\n","\n","         0.0       0.86      0.84      0.85      1121\n","         1.0       0.24      0.26      0.25       206\n","\n","    accuracy                           0.75      1327\n","   macro avg       0.55      0.55      0.55      1327\n","weighted avg       0.76      0.75      0.76      1327\n","\n","[[946 175]\n"," [152  54]].\n","\n","For defect:               precision    recall  f1-score   support\n","\n","         0.0       0.76      0.69      0.72       913\n","         1.0       0.43      0.53      0.48       414\n","\n","    accuracy                           0.64      1327\n","   macro avg       0.60      0.61      0.60      1327\n","weighted avg       0.66      0.64      0.65      1327\n","\n","[[629 284]\n"," [196 218]].\n"]}],"source":["print(f\"For cracks: {classification_report(actualCrack, predictionsCrack)}\\n{confusion_matrix(actualCrack, predictionsCrack)}.\\n\")\n","print(f\"For LRC: {classification_report(actualLrc, predictionsLrc)}\\n{confusion_matrix(actualLrc, predictionsLrc)}.\\n\")\n","print(f\"For defect: {classification_report(actualDefect, predictionsDefect)}\\n{confusion_matrix(actualDefect, predictionsDefect)}.\")"]},{"cell_type":"markdown","metadata":{"id":"IRay5Umn9sl7"},"source":["# Data Analysis\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g9WZEBxlMa7Z"},"outputs":[],"source":["len(filesTrain), len(filesTest), len(filesVal)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mAV1ulunMa7a"},"outputs":[],"source":["# fset = set(files)\n","# pre = \"C:/Users/Daniel Polania/OneDrive - University of Calgary/ENGG-680-Project/data/\"\n","# filenames = pd.read_excel(f\"{pre}Combined excel.xlsx\", usecols=\"A,B,C,D,R,U,V\")\n","# filenames = filenames.set_index(\"Filename\")\n","# import os\n","\n","# for file in fset:\n","#     attrib = file.split(\"_\")\n","#     name = attrib[0]+\".png\"\n","#     OSvalue = filenames.loc[name, \"OS\"]\n","#     newattrib = attrib[0] +\"_\"+ attrib[1] +\"_\"+ attrib[2][0] + str(OSvalue) + attrib[2][2:]\n","#     os.rename(f\"{datalocation}{file}\", f\"{datalocation}{newattrib}\")\n","\n","# print(\"Done rename.\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-IME2BDRMa7a"},"outputs":[],"source":["fileLabels = {}\n","for file in files:\n","    attrib = file.split(\"_\")\n","    fileLabels[file] = {\"va\":attrib[1], \"defectLabel\":attrib[2][2], \"crackLabel\":attrib[2][0],  \"lrcLabel\":attrib[2][1]}\n","fileLabels"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DdlCprMOMa7a"},"outputs":[],"source":["df = pd.DataFrame.from_dict(fileLabels, orient='index')\n","df.head()\n","df[\"defectLabel\"] = df.defectLabel.astype(np.uint16)\n","df[\"crackLabel\"] = df.crackLabel.astype(np.uint16)\n","df[\"lrcLabel\"] = df.lrcLabel.astype(np.uint16)\n","df[\"va\"] = df.lrcLabel.astype(np.float32)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3t5ZQJUWMa7a"},"outputs":[],"source":["describeDefects = df.defectLabel.value_counts()\n","describeDefects\n","plt.pie(describeDefects, labels=[0, 1], colors=[\"lightcoral\", \"peachpuff\"])\n","plt.title(\"Defect Distribution\")\n","plt.savefig(figuresPrefix+\"defectDistribution.png\")\n","plt.show()\n","\n","describeCracks = df.crackLabel.value_counts()\n","describeCracks\n","plt.pie(describeCracks, labels=[0, 1], colors=[\"lightcoral\", \"peachpuff\"])\n","plt.title(\"Crack Distribution\")\n","plt.savefig(figuresPrefix+\"crackDistribution.png\")\n","plt.show()\n","\n","describeLatcut = df.lrcLabel.value_counts()\n","describeLatcut\n","plt.pie(describeLatcut, labels=[0, 1], colors=[\"lightcoral\", \"peachpuff\"])\n","plt.title(\"LRC Distribution\")\n","plt.savefig(figuresPrefix+\"lrcDistribution.png\")\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LhFPUxyLMa7a"},"outputs":[],"source":["# set(list(df.defectLabel))\n","# imRsz\n","# X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size = 0.1, random_state=2)"]}],"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.2"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}